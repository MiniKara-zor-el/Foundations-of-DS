{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"M1_Assignment_04_ProbabilityAndStatisticsForDatascience_C.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"powered-thong"},"source":["# Applied Data Science and Machine Intelligence\n","## A program by IITM and TalentSprint\n","### Assignment 4: Probability and Statistics for Data Science"]},{"cell_type":"markdown","metadata":{"id":"innocent-wiring"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"actual-tiger"},"source":["At the end of the experiment, you will be able to\n","\n","* know a general idea on how probability plays an important role in data science.\n","* understand sampling distributions, sampling mean and standard-deviaton. Central limits theorem.\n","* use hypothesis testing: T Test, Z Test,  Chi-Square Test \n","* understand how confidence intervals are used.\n","* know more about correlation functions.\n","* use parameter estimation using MLE and Bayesian methods."]},{"cell_type":"markdown","metadata":{"id":"BNLA8HiKxQhc"},"source":["### Setup Steps:"]},{"cell_type":"code","metadata":{"id":"2YzfoPvJDiTX"},"source":["#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n","Id = \"\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AjoZJWGErxGf"},"source":["#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WBPPuGmBlDIN","cellView":"form"},"source":["#@title Run this cell to complete the setup for this Notebook\n","from IPython import get_ipython\n","\n","ipython = get_ipython()\n","  \n","notebook= \"M1_Assignment_04_ProbabilityAndStatisticsForDatascience_C\" #name of the notebook\n","\n","def setup():\n","#  ipython.magic(\"sx pip3 install torch\") \n","    from IPython.display import HTML, display\n","    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n","    print(\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","    \n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:        \n","        print(r[\"err\"])\n","        return None        \n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","    \n","    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n","              \"concepts\" : Concepts, \"record_id\" : submission_id, \n","              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n","              \"notebook\" : notebook,\n","              \"feedback_experiments_input\" : Comments,\n","              \"feedback_mentor_support\": Mentor_support}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","      if \"err\" in r:        \n","        print(r[\"err\"])\n","        return None   \n","      else:\n","        print(\"Your submission is successful.\")\n","        print(\"Ref Id:\", submission_id)\n","        print(\"Date of submission: \", r[\"date\"])\n","        print(\"Time of submission: \", r[\"time\"])\n","        print(\"View your submissions: https://adsmi.iitm.talentsprint.com/notebook_submissions\")\n","        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","        return submission_id\n","    else: submission_id\n","    \n","\n","def getAdditional():\n","  try:\n","    if not Additional: \n","      raise NameError\n","    else:\n","      return Additional  \n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    if not Complexity:\n","      raise NameError\n","    else:\n","      return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","  \n","def getConcepts():\n","  try:\n","    if not Concepts:\n","      raise NameError\n","    else:\n","      return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","  \n","  \n","# def getWalkthrough():\n","#   try:\n","#     if not Walkthrough:\n","#       raise NameError\n","#     else:\n","#       return Walkthrough\n","#   except NameError:\n","#     print (\"Please answer Walkthrough Question\")\n","#     return None\n","  \n","def getComments():\n","  try:\n","    if not Comments:\n","      raise NameError\n","    else:\n","      return Comments\n","  except NameError:\n","    print (\"Please answer Comments Question\")\n","    return None\n","  \n","\n","def getMentorSupport():\n","  try:\n","    if not Mentor_support:\n","      raise NameError\n","    else:\n","      return Mentor_support\n","  except NameError:\n","    print (\"Please answer Mentor support Question\")\n","    return None\n","\n","def getAnswer():\n","  try:\n","    if not Answer:\n","      raise NameError \n","    else: \n","      return Answer\n","  except NameError:\n","    print (\"Please answer Question\")\n","    return None\n","  \n","\n","def getId():\n","  try: \n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup \n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup() \n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aGZilB81a1yH"},"source":["#### Importing necessary packages for this assignment.\n","Lets first import the necessary packages required for this notebook."]},{"cell_type":"code","metadata":{"id":"iXGo3puj_I2Q"},"source":["# Importing the necessary packages\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy\n","import pandas as pd\n","from statsmodels.stats import weightstats as stests # Necessary for z-test\n","import seaborn as sns # Required for some of the plotting tasks.\n","\n","# Packages necessary for bayesian inference\n","from sklearn.linear_model import BayesianRidge"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rOsam8JmTB1f"},"source":["### Importance of Probability and Statistics in Data Science\n","\n","Making estimates and predictions from a large corpus of real world data is often the primary challenge a data scientist faces. Estimates can be made using statistical methods which are heavily dependent of probability theory. Thus, probability and statistics are key tools that are present at the disposal of a data scientist. Let us now understand the concept of data and how it relates to probability and statistics.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lkVEaAcORrZv"},"source":["### What is data?\n","\n","Data is the information that is collected through observations. The data can be a collection of facts (numbers, words, measurements, observations, etc) that has been translated into a form that computers can process. Data provides us with various types of information like - \n","1. If two variables are inter-related?\n","2. Helps in forecasting different outcomes based on given prior.\n","3. Helps in determining various patterns present in large amounts of data. Such patterns can also be utilized to detect anomoulous occurances. \n","\n","Large amounts of data are often represented as a distribution with different properties like mean, varience, etc. Such distributions are called sampling distributions."]},{"cell_type":"markdown","metadata":{"id":"9sUsbLja3zEU"},"source":["### Sampling Distributions\n","\n","A statistic, such as the sample mean or the sample standard deviation, is a number computed from large amounts of given data which is often thought to be a \"sample\". Since a sample is random, every statistic is a random variable: it varies from sample to sample in a way that cannot be predicted with certainty. As a random variable it has a mean, a standard deviation, and a probability distribution. The probability distribution of a statistic is called its sampling distribution. "]},{"cell_type":"markdown","metadata":{"id":"0KC3wwQq621t"},"source":["#### Sample Mean and Sample Variance.\n","\n","Lets say that we wish to estimate $\\mu$ of a population. We will take random $n$ instances from the population and call it a sample. We will take multiple such samples, each of size $n$ and compute the mean $\\overline{x}$ of each one. Therefore, we will have $n$ means. The sample mean $\\overline{x}$ is a random variable: it varies from sample to sample in a way that cannot be predicted with certainty. We will denote this random variable as $\\overline{X}$. Please note that $\\overline{X}$ is the random variable representing sample mean (each sample is a collection of $n$ instances selected randomly). The random variable $\\overline{X}$ will have a mean $\\mu_\\overline{X}$ and standard deviation $\\sigma_\\overline{X}$. \n","\n","<br>\n","\n","#### Example\n","Q1. A  beach volleyball team consists of three players who weigh 100, 150, 200 pounds. Find all possible random samples without replacement of size two and compute the sample mean for each one. Use them to find the probability distribution, the mean, and the standard deviation of the sample mean $\\overline{X}$.\n","\n","Ans - Let us find all the pairs of players (each sample). $(100, 150), (100, 200), (150, 100), (150, 200), (200, 100), (200, 150)$. Here the sample size $n=2$. The means of each of these samples (denoted by $\\overline{x}$) are $125, 150, 125, 175, 150, 175$. Therefore, the random variable $\\overline{X}$ can take $125, 150, 175$ as values. Now each of values are equally likely, therefore $p(\\overline{X} = 125) = p(\\overline{X} = 150) = p(\\overline{X} = 175) = \\frac{1}{3}$. The mean of a discrete random variable is given by,\n","\n","$\\mu_{\\overline{X}=\\overline{x}} = \\Sigma \\overline{x}p(\\overline{x})$\n","\n","Therefore in our case, $\\mu_\\overline{X} = \\frac{1}{3}.100 + \\frac{1}{3}.150 + \\frac{1}{3}.200 = 150$\n","\n","Let us now find the variance of $\\overline{X}$, i.e. $\\sigma_\\overline{X}^2$.\n","\n","$\\sigma_\\overline{X}^2 = \\Sigma_\\overline{x} (\\overline{x} - \\mu_\\overline{X})^2 . p(\\overline{x}) = (100-150)^2.\\frac{1}{3} + (150-150)^2.\\frac{1}{3} + (200-150)^2.\\frac{1}{3} = \\frac{5000}{3} = 1666.6667$\n","\n","Therefore, $\\sigma_\\overline{X} = 40.824$\n","\n","Now let us check the $\\mu$ and $\\sigma$ of the total population. \n","\n","$\\mu = \\frac{(100 + 150 + 200)}{3} = 150 = \\mu_{\\overline{X}}$. Therefore, mean of the total population is equal to the mean of the Sample Mean!\n","\n","Similarly, standard deviation $\\sigma$ of the total population = $50$\n","If we look closely then, $50/\\sqrt{2} \\approx 40.824 $. Therefore, $\\sigma_\\overline{X} = \\frac{\\sigma}{\\sqrt{n}}$. Here $n$ is the size of each sample!\n","\n","To summarize, we can estimate the mean of a total population by finding the mean of sample mean (mean of multiple samples randomly sampled from the population) and also estimate the standard deviation of the population by finding the standard deviation of the sample mean.\n","\n","$\\mu = \\mu_{\\overline{X}}$, $\\sigma_\\overline{X} = \\frac{\\sigma}{\\sqrt{n}}$, where $n$ is the number of instances present in each sample. **These two properties are extremely important as it effectively allows us not to estimate the mean and standard-deviation of a population by using randomly sampled instances.** While, knowing $\\mu$ and $\\sigma$ of a population is highly benificial, estimating the shape of the distribution is also important. One of the most used distribution is the Gaussian or the normal distribution. We now look at a central limit theorem that helps us map $\\mu_\\overline{X}$ and $\\sigma_\\overline{X}$ to a normal distribution."]},{"cell_type":"markdown","metadata":{"id":"_C0ROenzLWNY"},"source":["#### The central limit theorem.\n","\n","According to this theorem, for samples of size 30, i.e., $(n > 30)$ or more, the sample mean is approximately normally distributed, with mean $\\mu_\\overline{X} =\\mu$ and standard deviation $\\sigma_\\overline{X} = \\frac{\\sigma}{\\sqrt{n}}$, where $n$ is the sample size. The larger the sample size, the better the approximation.\n","\n","Let us check this theorem using a simply python code to plot the means of random sample (varying $n$ for each sample). We will select the random numbers between $-100$ and $100$. We will use $n=\\{1, 10, 30, 60, 100, 150\\}$. For each $n$ we will choose $1000$ samples from the same population (between $-100$ and $100$ denotes the population). \n","\n","\n","For more information about [central limit theorem](https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_probability/BS704_Probability12.html)"]},{"cell_type":"code","metadata":{"id":"UwtjBqFibHgP"},"source":["# number of instances in a sample (n)\n","num = [1, 10, 30, 60, 100, 150] \n","\n","# list of sample means\n","means = [] \n","\n","# Generating 1, 10, 30, 60, 100, 150 random numbers from -100 to 100 in each sample\n","# We will generate 1000 such sample for each n between -100 to 100 (population). Each sample will have 1 or 10 or 30 or 60 or 100 or 150 numbers.\n","# taking their mean and appending it to list means.\n","for j in num:\n","    x = [np.mean(np.random.randint(-100, 100, j)) for _i in range(1000)]\n","    means.append(x)\n","\n","k = 0\n"," \n","# plotting all the means in one figure\n","fig, ax = plt.subplots(2, 3, figsize =(16, 16))\n","for i in range(0, 2):\n","  for j in range(0, 3):\n","    # Histogram for each x stored in means\n","    ax[i, j].hist(means[k], 100, density = True)\n","    ax[i, j].set_title(label = f\"n = {num[k]}\")\n","    k = k + 1\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Na0EdPsLeCrh"},"source":["As we can see, for the approximation towards a normal distribution improves with improving sample size $n$. \n","\n","We now check how estimating population parameters can help in the process of hypothesis testing."]},{"cell_type":"markdown","metadata":{"id":"mLvmm55tePeV"},"source":["### Hypothesis Testing\n","\n","A hypothesis is based on available information and the investigator's belief about the population parameters. The process of hypothesis testing involves setting up two competing hypotheses, the null hypothesis and the alternate hypothesis. One selects a random sample (or multiple samples when there are more comparison groups), computes summary statistics and then assesses the likelihood that the sample data support the research or alternative hypothesis. In other words, a hypothesis test evaluates two mutually exclusive statements about a population to determine which statement is best supported by the sample data. It helps us claim a finding to be statistically significant (can often be useful while claiming validity of experimental results). Let us now see four most widely used hypothesis testing methods. \n","\n","#### P-Value\n","\n","The P value, or calculated probability, is the probability of finding the observed. A p value is used in hypothesis testing to help you support or reject the null hypothesis. The p value is the evidence against a null hypothesis. The smaller the p-value, the stronger the evidence that you should reject the null hypothesis (accept the research hypothesis). Generally p-value $<$ than $0.05$ is considered to be enough to reject the null hypothesis.\n","\n","\n","![img](https://cdn.extras.talentsprint.com/ADSMI/Images/pvalue.png)\n"]},{"cell_type":"markdown","metadata":{"id":"o8FUfjl9wPyR"},"source":["The P-value will be extensively used by the following tests reported below."]},{"cell_type":"markdown","metadata":{"id":"T6YNQ9OWh0Jd"},"source":["#### T-Test\n","A t-test is a type of test to check if there is a significant difference between the means of two groups that may be related in certain features. It is generally used when the data sets, like the set of data recorded as outcome from flipping a coin a 100 times, would follow a normal distribution and may have unknown variances. \n","\n","\n","\n","**One sample student T-test:**\n","One Sample t-test (single sample t-test) is used to compare the sample mean (a random sample from a population) with the specific value (hypothesized or known mean of the population). In the following example, we check a set of ages $\\{ 10,20,35,50,28,40,55,18,16,55,30,25,65,18,43,23,21,20,19,70\\}$. We assume the null hypothesis to be mean of the ages $= 50$. The original mean is 33.05. Therefore, the null hypothesis should ideally be rejected. Lets check by using a python code with ``scipy.stats.ttest_1samp``.\n","\n"]},{"cell_type":"code","metadata":{"id":"uN-SumNKsXd6"},"source":["# Define a sample data of ages.\n","ages=[10,20,35,50,28,40,55,18,16,55,30,25,65,18,43,23,21,20,19,70]\n","\n","ages_mean = np.mean(ages)\n","\n","print(\"Mean of ages: \", ages_mean)\n","\n","# Lets hypothize is the average age of a person is 50 in the given population.\n","# We already know that this is not true. The mean age is around 33. Therefore, p-value should be less than 0.05\n","null_hypothesis = 50 \n","\n","tset, pval = scipy.stats.ttest_1samp(ages, null_hypothesis)\n","print(\"P-value: \", pval)\n","\n","if pval < 0.05:    # If p-value is less than 0.05 we will reject the null hypothesis\n","    print(\"We are rejecting null hypothesis i.e,average age is not \"+str(null_hypothesis))\n","else:\n","    print(\"We are accepting null hypothesis i.e,average age is \"+str(null_hypothesis))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UKJMA-xfyDbY"},"source":["In the above code, please change the null hypothesis age to $33.05$ and check the result!"]},{"cell_type":"markdown","metadata":{"id":"Huv4EeYAyYzR"},"source":["**Two sample student T-test:**\n","The independent samples T-test or 2-sample T-test compares the means of two independent groups in order to determine whether there is statistical evidence that the population means are significantly different or same for both sets of data. In simpler terms, whether both the data is from the same population or different? "]},{"cell_type":"code","metadata":{"id":"dUSxdFEGzJwe"},"source":["# We will first create two dummy data. \n","# data1 are all even\n","# data2 is data1 plus epsilon (ideally these should be from the same distributions)\n","print (\"CASE 1\")\n","data1 = [2, 4, 6, 8, 10, 12, 14, 16, 18]\n","data2 = [2.0001, 4.0002, 6.00001, 8.00001, 10.0001, 12.00001, 14.09001, 16.0001, 18.0001]\n","\n","print (\"Data 1: \", data1)\n","print (\"Data 2: \", data2)\n","stat, p = scipy.stats.ttest_ind(data1, data2)\n","print('P Value is :',p)\n","if p > 0.05:\n","    print('Both the datas are from the same distribution')\n","else:\n","    print('Both the datas are from different distributions')\n","\n","# Let us now check another case. In this, we make data2 to be very small \n","print (\"---------------------------------------\")\n","print (\"CASE 2\")\n","data1 = [2, 4, 6, 8, 10, 12, 14, 16, 18]\n","data2 = [ 0.0001, 0.0000002, 0.000005, 0.0002, 0.0009, 0.000005, 0.00003, 0.000001]\n","\n","print (\"Data 1: \", data1)\n","print (\"Data 2: \", data2)\n","stat, p = scipy.stats.ttest_ind(data1, data2)\n","print('P Value is :',p)\n","if p > 0.05:\n","    print('Both the datas are from the same distribution')\n","else:\n","    print('Both the datas are from different distributions')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KSMWCKS0zJEV"},"source":["#### Z-Test\n","While the T-test is designed for samples with lower sample size, Z-test is often used for larger sample sizes. This is because, Z-test considers the samples to be normally distributed according to the central limits theorem.\n","\n","* A Z-test is a statistical test to determine whether two population means are different when the variances are known and the sample size is large.\n","* It can be used to test hypotheses in which the Z-test follows a normal distribution.\n","\n","Similar to T-test, the Z-test can also be divided into two cases. \n","The one-sample and a two-sample case. In one sample Z-test, we try to find whether the mean of a data group is the population mean or not."]},{"cell_type":"code","metadata":{"id":"MHzH20mC36vV"},"source":["# We take a population of a guassian distribution of mean 10.0 and standard dev 1.0\n","\n","# Population mean\n","mean_population = 10.0\n","\n","# Let us take a sample of 10000 points from the same distribution\n","# We need to check whether the mean of the sample and the mean of the population are close or not\n","# The null hypothesis is they are not close. \n","# If the p-value is greater than 0.05 we can reject the null hypothesis.\n","# Please note that the parameter \"value\" takes in the mean of sample under null hypothesis. \n","# This is significant because mean(null hypothesis) != mean(population). Therefore if p > 0.05 it rejects the null hypothesis. \n","sample = np.random.normal(10.0, 1.0, 10000)\n","ztest, pval = stests.ztest(sample, value=mean_population)\n","\n","print (\"P-Value: \", pval)\n","\n","if pval > 0.05:\n","  print (\"Rejecting Null Hypothesis: The mean of sample and the mean of population are close. \\nThe sample and the population can be thought to have similar characteristics!\")\n","else:\n","  print (\"The mean of population and the sample are very different! Null hypothesis wins!\")\n","\n","print (\"------------------------------\")\n","print (\"Let us now check the means directly\")\n","print (\"Mean of population: \", mean_population)\n","print (\"Mean of sample: \", np.mean(sample))\n","print (\"If the means are close to each other the p-val is below 0.05 and vice versa.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oKIkFoLtBIga"},"source":["Try out the same by changing the $\\mu$ and $\\sigma$ values of the sample distribution in the above code."]},{"cell_type":"markdown","metadata":{"id":"PH4O9dni_Igt"},"source":["For 2-sample Z-test, we do a very similar experiment to the  2-sample T-test one. The main goal of this test is to check if given two samples from unknown populations, whether their sample means are equal or not. "]},{"cell_type":"code","metadata":{"id":"KkUGWWz6Im4K"},"source":["# Let us take different numbers of points (different sized samples) from the same population. We define the population as a normal distribution which mean = 0 and\n","# standard deviation = 1.\n","# Sample 1 will have a size of 10,000 and sample 2 will have a size of 1,000\n","\n","sample1 = np.random.normal(0.0, 1.0, 10000)\n","sample2 = np.random.normal(0.0, 1.0, 1000)\n","\n","# Null hypothesis is the samples are from different distribution. Thus the difference of means between the two distribution is not 0. \n","# The parameter value expects the the difference between mean of x1 and mean of x2 under the Null hypothesis. This value cannot be 0 because null hypothesis states that\n","# the means are different. Therefore if we pass 0 here, then p < 0.05 will be accepting the null hypothesis and vice versa\n","ztest, pval = stests.ztest(sample1, sample2, value=0, alternative='two-sided')\n","print (\"P-value: \", pval)\n","\n","if pval > 0.05:\n","  print (\"Rejecting null hypothesis: The two samples have the same sample mean!\")\n","else:\n","  print (\"Accepting null hypothesis: The two distributions have different sample means\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kZHuIkvzMFfw"},"source":["Please change the distributions of the samples and check the effect!"]},{"cell_type":"markdown","metadata":{"id":"ECb6fjnEQgy4"},"source":["#### Chi-Square Test\n","\n","The test is applied when we have two categorical variables from a single population. It is used to determine whether there is a significant association between the two variables.\n","\n","For example, in a survey amongs cricket players, players might be classified by their roles (batsman or bowler) and their average career span (5 years, 10 years, 15). We could use a chi-square test for independence to determine whether your playing position is related with how long you play. \n","\n","We provide a simple python code for the Chi-Square test."]},{"cell_type":"code","metadata":{"id":"d_8VMUmXSKAQ"},"source":["# In the array players, we represent bowlers as 1 and batsman as 2\n","# In the array career, we denote the number of years they played cricket\n","\n","# The null hypothesis is that they are independent. If p > alpha then we accept null hypothesis, else we reject it.\n","# Alpha is calculated as 1-p \n","# Alpha can also be set to 0.05 like done in other examples \n","\n","players = np.array([2, 2, 1, 2, 1, 1, 2, 2, 1, 1, 2, 2])\n","career = [10, 10, 5, 10, 5, 5, 15, 10, 5, 5, 10, 10]\n","\n","table = [career, players]\n","stat, p, dof, expected = scipy.stats.chi2_contingency(table)\n","\n","# In chi-square test, we find alpha = 1-p for checking the statistical significance.\n","p = 1-p\n","alpha = 1-p\n","print('P value:', p)\n","print ('Alpha value: ', alpha)\n","if alpha >= p:\n","    print('Rejecting null hypothesis:  playing career and the are dependent!')\n","else:\n","    print('Accepting null hypothesis: they are independent!')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aFo8XF80jZZ9"},"source":["So far we have covered the concepts of sampling distributions as well as hypothesis testing. However, the sample mean and sample variance can never be fully estimated, thus the mean and varience of a complete population also remains uncertain. Thus, the concept of confidence intervals are introduced which bounds this uncertainity."]},{"cell_type":"markdown","metadata":{"id":"AxFtZkfRaCSu"},"source":["### Confidence Intervals\n","\n","A confidence interval is a bound on the estimate of a population variable. It is an interval statistic used to quantify the uncertainty on an estimate. The confidence interval provides bounds on population parameters, such as a mean, standard deviation, etc. From a machine learning point of view, we use confidence intervals in the presentation of the skill of a predictive model.\n","\n","For example, a confidence interval could be used in presenting the skill of a classification model, which could be stated as:\n","\n","Given the sample, there is a 95% likelihood that the range x to y covers the true model accuracy.\n","\n","#### Formula for finding confidence interval of a normally distributed data - \n","\n","$CI = \\overline{X} \\pm Z^* \\frac{\\sigma}{\\sqrt{n}}$\n","\n","Here,\n","* CI = the confidence interval\n","* $\\overline{X}$ = the population mean\n","* $Z^*$ = the critical value of the z-distribution\n","* $\\sigma$ = the population standard deviation\n","* $\\sqrt{n}$ = the square root of the population size\n","\n","Let us now check simple python codes which helps us in determining the confidence intervals of a given data.\n","\n","\n"," "]},{"cell_type":"code","metadata":{"id":"l8iBFxKEtiNx"},"source":["#define sample data\n","# We take 50 random points between 10 and 30\n","data = np.random.randint(10, 30, 50)\n","\n","#create 95% confidence interval for population mean\n","ci_l, ci_h = scipy.stats.norm.interval(alpha=0.95, loc=np.mean(data), scale=scipy.stats.sem(data))\n","\n","print (\"Mean of the given data: \", np.mean(data))\n","\n","print (\"The 95% confidence interval for the true population mean is ({}, {}).\".format(ci_l, ci_h))\n","\n","#create 99% confidence interval for population mean \n","ci_l, ci_h = scipy.stats.norm.interval(alpha=0.99, loc=np.mean(data), scale=scipy.stats.sem(data))\n","print (\"The 99% confidence interval for the true population mean is ({}, {}).\".format(ci_l, ci_h))\n","\n","#create 99.99% confidence interval for population mean \n","ci_l, ci_h = scipy.stats.norm.interval(alpha=0.9999, loc=np.mean(data), scale=scipy.stats.sem(data))\n","print (\"The 99.99% confidence interval for the true population mean is ({}, {}).\".format(ci_l, ci_h))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"djw6nOQqu44e"},"source":["### Correlation Function\n","\n","A correlation function is a function that gives the statistical correlation between random variables, contingent on the spatial or temporal distance between those variables. \n","\n","If one considers the correlation function between random variables representing the same quantity measured at two different points, then this is often referred to as an autocorrelation function, which is made up of autocorrelations. Correlation functions of different random variables are sometimes called cross-correlation functions to emphasize that different variables are being considered and because they are made up of cross-correlations.\n","\n","\n","Correlation functions are a useful indicator of dependencies as a function of distance in time or space, and they can be used to assess the distance required between sample points for the values to be effectively uncorrelated. In addition, they can form the basis of rules for interpolating values at points for which there are no observations.\n","\n","For possibly distinct random variables $X(s)$ and $Y(t)$ at different points $s$ and $t$, the correlation function is\n","\n","<center> $C(s, t) = Corr(X(s), Y(t))$ </center>\n","\n","Here $Corr$ functions relates to the Correlation function which is defined as,\n","\n","<center>$Corr (X(s), Y(t)) = \\frac{Cov(X(s), Y(t))}{\\sigma_{X(s)} .\\sigma_{Y(t)}} $</center>"]},{"cell_type":"markdown","metadata":{"id":"f_J7INCp12CN"},"source":["If the random variables $X(s)$ is a vector with $n$ elements and $Y(t)$ with $q$ element then an $n \\times q$ matrix of correlation functions is defined with $(i,j)$ element.\n","\n","$C_{i,j}(s, t) = Corr(X_i(s), Y_j(t))$. \n","\n","The correlation function is available in Numpy and is shown in the following code snippet. The value from the correlation function being close to $0$ means there is no correlation between the random variables while nearing $-1$ or $1$ means they are highly correlated."]},{"cell_type":"code","metadata":{"id":"0D_YoFfx1kBx"},"source":["arrayA = [ .1, .2, .4 ]\n","arrayB = [ .3, .1, .3 ]\n","\n","cor = np.corrcoef( arrayA, arrayB )[0,1]\n","print (\"Correlation between A and B is: \", cor)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jfi1yu1Cia5C"},"source":["Finally, we see how prior information extracted from a given set of data can be utilized for estimating future occurances. We look at two standard estimation techniques. The Maximum Likelihood Estimation (MLE) and the Bayesian Estimation. We describe both of them in details."]},{"cell_type":"markdown","metadata":{"id":"A-K0gPNi83RU"},"source":["### Maximum Liklihood Estimation\n","\n","Maximum Likelihood Estimation(MLE), is a traditional probabilistic approach that can be applied to data belonging to any distribution, i.e., Normal, Poisson, Bernoulli, etc. With prior assumption or knowledge about the data distribution, Maximum Likelihood Estimation helps find the most likely-to-occur distribution parameters. \n","\n","For example, let us say we have data that is assumed to be normally distributed, but we do not know its mean and standard deviation parameters. Maximum Likelihood Estimation iteratively searches the most likely mean and standard deviation that could have generated the distribution.\n","\n","In Maximum Likelihood Estimation, we wish to maximize the conditional probability of observing the data ($X$) given a specific probability distribution and its parameters ($\\theta$).\n","\n","We state it formally as,\n","\n","$P(X ; \\theta)$\n","Where $X$ is, in fact, the joint probability distribution of all observations from the problem domain from $1$ to $n$ which can be written as $P(x_1, x_2, x_3, â€¦, x_n ; \\theta)$.\n","\n","This resulting conditional probability is referred to as the likelihood of observing the data given the model parameters and written using the notation $L(X, \\theta)$ to denote the likelihood function. \n","\n","The joint probability distribution can be restated as the multiplication of the conditional probability for observing each example given the distribution parameters. Therefore the maximizing likelihood function can be stated as, \n","\n","$Maximize \\; \\Pi_{i=1}^n P(x_i ; \\theta)$\n","\n","However, multiplying probabilities, each of which is less than or equal to 1, will mean extremely small numbers which can be unstable. Thus, this function is modified using logarithmic transform. The function becomes,\n","\n","$Maximize \\; \\Sigma_{i=1}^n log(P(x_i ; \\theta)) = Minimize \\; (-\\Sigma_{i=1}^n log(P(x_i ; \\theta))) $\n","\n","The Maximum Likelihood Estimation framework can be used as a basis for estimating the parameters of many different machine learning models for regression and classification predictive modeling. Below we check a direct usage of MLE for regression task. \n","\n","To know more about **Maximum Likelihood Estimation** click [here](https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1)\n"]},{"cell_type":"code","metadata":{"id":"BMS2B3HvZbbi"},"source":["# create dummy observations\n","x = np.linspace(0, 30, 1000)\n","\n","# create a set of normally distributed points to add variation to the ground-truth\n","e = np.random.normal(10, 5, 1000)\n","\n","# generate dummy ground-truth for the sample points.\n","# In real world scenerio the func looks like: y = const + beta*x + var\n","# We put const=10, beta=4 and samples from e for adding the variance.\n","y = 10 + 4*x + e\n","\n","# Put the data into the pandas data frame\n","df = pd.DataFrame({'x':x, 'y':y})\n","\n","# Plot first 4 samples. Here x is observation and y is ground-truth\n","print (df.head())\n","print (\"---------------------\")\n","\n","# visualize data distribution\n","sns.regplot(x='x', y='y', data = df)\n","plt.show()\n","\n","print (\"----------------------\")\n","\n","# Finding the negative log likelihood given a set of parameters. \n","# We find three parameters, const, beta and std_dev\n","def MLE_Norm(parameters):\n","  const, beta, std_dev = parameters\n","  pred = const + beta*x + std_dev\n","\n","  LL = np.sum(scipy.stats.norm.logpdf(y, pred, std_dev))\n","  neg_LL = -1*LL\n","  return neg_LL\n","\n","# Optimizing the MLE model. Minimizing the negative log likelihood.\n","mle_model = scipy.optimize.minimize(MLE_Norm, np.array([2,2,2]), method='L-BFGS-B')\n","print (\"Information about the model: \")\n","print (mle_model)\n","\n","print (\"----------------------\")\n","# Final parameters that are predicted.\n","print (\"const = \", mle_model['x'][0], \"|| Actual = \", 10)\n","print (\"beta = \", mle_model['x'][1], \"|| Actual = \", 4)\n","print (\"std_dev = \", mle_model['x'][2], \"|| Actual = \", 5)\n","print (\"----------------------\")\n","\n","# Lets create two manual test data that can be used in the next experiment involving bayesian inference.\n","x_test = np.array([1, 2, 3])\n","e1 = np.random.normal(10, 5, 3)\n","\n","# Ground-truth for test that we only use for checking\n","y_test = 10 + 4*x_test + e1\n","\n","# Prediction using the MLE model that was fitted previously on the data\n","y_pred = mle_model['x'][0] + mle_model['x'][1]*x_test + np.array([mle_model['x'][2], mle_model['x'][2], mle_model['x'][2]])\n","\n","print (\"Expected Output: \", y_test)\n","print (\"Predicted Output: \", y_pred)                                                             "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dz1hskr3f5Lp"},"source":["### Bayesian Method of Parameter Estimation\n","\n","The Bayes theorem states that,\n","\n","$P(A|B) = \\frac{P(B|A). P(A)}{p(B)}$\n","\n","Here,  $A$ and $B$ are events, $P(A)$ is the probability of occurrence of $A$, and $P(A|B)$ is the probability of $A$ to occur given that event $B$ has already occurred. $P(B)$, the probability of event $B$ occurring cannot be $0$ since it has already occurred.\n","\n","This concept is used for Bayesian parameter estimation. The simple formula states, \n","\n","$Posterior = \\frac{Likelihood*Prior}{Normalization}$\n","\n","Here, \n","\n","* Posterior: It is the probability of an event to occur; say, $A$, given that another event; say, $B$ has already occurred. i.e., $P(A | B)$.\n","* Prior: It is the probability of an event $A$ has occurred prior to another event. i.e., P(A)\n","* Likelihood: It is a likelihood function in which some parameter variable is marginalized."]},{"cell_type":"code","metadata":{"id":"Kaatcutjn8q_"},"source":["# Creating the same dataset as done in the previous example\n","x = np.linspace(0, 30, 1000)\n","\n","# create a set of normally distributed points to add variation to the ground-truth\n","e = np.random.normal(10, 5, 1000)\n","\n","# generate dummy ground-truth for the sample points.\n","# In real world scenerio the func looks like: y = const + beta*x + var\n","# We put const=10, beta=4 and samples from e for adding the variance.\n","y = 10 + 4*x + e\n","\n","# Reshaping because the BayesianRidge function accepts 2D shapes only!\n","x = np.reshape(x, [ x.shape[0], 1])\n","\n","# Creating and training model\n","model = BayesianRidge()\n","model.fit(x, y)\n","\n","# We take the same test data as taken during the MLE estimation.\n","# Please check the MLE estimation example to get the estimated output as well as the expected output.\n","X_test = np.array([1, 2, 3])\n","X_test = np.reshape(X_test, (X_test.shape[0], 1))\n","\n","# Model making a prediction on test data\n","prediction = model.predict(X_test)\n","\n","print (\"Predicted output from Bayesian inference is: \", prediction)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2jotNVAis9s2"},"source":["As we can see, both the MLE predicted output and the Bayesian estimated output are similar. You can also check the expected outputs in the cell containing the MLE codes. Please use your own dummy datasets and test the results for both of the parameter estimation techniques."]},{"cell_type":"markdown","metadata":{"id":"L5wJgz1-PDEn"},"source":["### Ungraded Questions\n","1. Using a code, find the confidence interval of $95\\%$ from a population of 10000 samples drawn from a normal distribution with $\\mu=0$ and $\\sigma=10$.\n","2. Using an appropriate test, check if the following data are from the same distribution or not. \n","\n","\n","\n","> Data 1:$\\{3.097,  4.179,  6.985, -2.266,  8.076, 0.518, 9.861, -9.265, -5.353, 7.744 \\}$.\n","\n","\n","> Data 2: $\\{3.097,  4.179,  6.985, -2.266,  8.076,   0.518,\n","  9.861, -9.265, -5.353,   7.744, \\}$\n"]},{"cell_type":"markdown","metadata":{"id":"VHfHdGCP_n6Y"},"source":["### Please answer the questions below to complete the experiment:\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"63QxuznPK9Hv"},"source":["**Question:** Footballers playing in the Barclays Premier League are weighed and the measurements are recorded. Each teams first choice playing $11$ is considered for this excercise and is considered a sample. The sample mean has a mean of $\\mu_\\overline{X}$ which is calculated to be 80 KG. The standard deviation $\\sigma_\\overline{X}$ of the sample mean is calculated to be $9$ KG."]},{"cell_type":"code","metadata":{"id":"VgSwVENIPcM6"},"source":["# @title Calculate the mean and standard deviation of the population. { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Answer = \"\" #@param [\"\", \"Mean = 265.32 KG ; Standard Deviation = 9 KG\",\"Mean = 80 KG ; Standard Deviation = 29.84 KG\",\"Mean = 24.12 KG ; Standard Deviation = 9 KG\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NMzKSbLIgFzQ"},"source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DjcH1VWSFI2l"},"source":["#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"\" #@param {type:\"string\"}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4VBk_4VTAxCM"},"source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XH91cL1JWH7m"},"source":["#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z8xLqj7VWIKW"},"source":["#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FzAZHt1zw-Y-","cellView":"form"},"source":["#@title Run this cell to submit your notebook for grading { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id = return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":null,"outputs":[]}]}