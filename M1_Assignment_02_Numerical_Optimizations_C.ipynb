{"nbformat":4,"nbformat_minor":5,"metadata":{"colab":{"name":"M1_Assignment_02_Numerical_Optimizations_C.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"powered-thong"},"source":["# Applied Data Science and Machine Intelligence\n","## A program by IIT Madras and TalentSprint\n","### Assignment 2:  Optimization Algorithms"],"id":"powered-thong"},{"cell_type":"markdown","metadata":{"id":"hungry-accident"},"source":["## Learning Objectives\n","\n","At the end of the experiment, you will be able to\n","\n","* understand optimization, and differentiate between convex and non-convex optimization\n","* understand unconstrained and constrained optimizations\n","* understand gradient descent methods"],"id":"hungry-accident"},{"cell_type":"markdown","metadata":{"id":"f9xrmGVLBB_n"},"source":["## Information"],"id":"f9xrmGVLBB_n"},{"cell_type":"markdown","metadata":{"id":"sublime-obligation"},"source":["### Optimization\n","\n","Optimization is a complex process of sequential and iterative procedures that can make a system work at its best effectiveness.\n","The system could be Physical or Software based. Some common examples are Motor vehicle traction control system, a Fighter aircraft and its missile system, a Business model, Economic and budget planning system, Human resource Management, Traffic monitoring network, Internet routing devices. The list is practically endless. Optimization is an important tool in decision science and in the analysis of physical systems.\n","\n","\n","Optimization deals with an Objective, a quantitative measure of the performance of the system under study.\n","Unconstrained Optimization does not set limits or constraints on the ojective function whereas Constrained Optimization has constraints.\n","\n","We must first identify an objective which depend on system variables that affect it.\n","Our goal is to find values of the variables that optimize the objective.\n","\n","Based on the problem domain, this objective could be profit, time, potential energy, or any quantity or combination of quantities that can be represented by a single number.\n","The following figure explaines the relationship between Learning Rate and the Loss function\n","![img](https://miro.medium.com/max/700/1*rcmvCjQvsxrJi8Y4HpGcCw.png)"],"id":"sublime-obligation"},{"cell_type":"markdown","metadata":{"id":"BNLA8HiKxQhc"},"source":["### Setup Steps:"],"id":"BNLA8HiKxQhc"},{"cell_type":"code","metadata":{"id":"2YzfoPvJDiTX"},"source":["#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n","Id = \"\" #@param {type:\"string\"}"],"id":"2YzfoPvJDiTX","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AjoZJWGErxGf"},"source":["#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"\" #@param {type:\"string\"}"],"id":"AjoZJWGErxGf","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WBPPuGmBlDIN","cellView":"form"},"source":["#@title Run this cell to complete the setup for this Notebook\n","from IPython import get_ipython\n","\n","ipython = get_ipython()\n","  \n","notebook= \"M1_Assignment_02_Numerical_Optimizations_C\" #name of the notebook\n","\n","def setup():\n","#  ipython.magic(\"sx pip3 install torch\")  \n","    ipython.magic(\"sx wget https://cdn.extras.talentsprint.com/ADSMI/Datasets/SIMPLEPENDULUMOSCILLATIONDATA.csv\")\n","    from IPython.display import HTML, display\n","    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n","    print(\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","    \n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:        \n","        print(r[\"err\"])\n","        return None        \n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","    \n","    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n","              \"concepts\" : Concepts, \"record_id\" : submission_id, \n","              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n","              \"notebook\" : notebook,\n","              \"feedback_experiments_input\" : Comments,\n","              \"feedback_mentor_support\": Mentor_support}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","      if \"err\" in r:        \n","        print(r[\"err\"])\n","        return None   \n","      else:\n","        print(\"Your submission is successful.\")\n","        print(\"Ref Id:\", submission_id)\n","        print(\"Date of submission: \", r[\"date\"])\n","        print(\"Time of submission: \", r[\"time\"])\n","        print(\"View your submissions: https://adsmi.iitm.talentsprint.com/notebook_submissions\")\n","        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","        return submission_id\n","    else: submission_id\n","    \n","\n","def getAdditional():\n","  try:\n","    if not Additional: \n","      raise NameError\n","    else:\n","      return Additional  \n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    if not Complexity:\n","      raise NameError\n","    else:\n","      return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","  \n","def getConcepts():\n","  try:\n","    if not Concepts:\n","      raise NameError\n","    else:\n","      return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","  \n","  \n","# def getWalkthrough():\n","#   try:\n","#     if not Walkthrough:\n","#       raise NameError\n","#     else:\n","#       return Walkthrough\n","#   except NameError:\n","#     print (\"Please answer Walkthrough Question\")\n","#     return None\n","  \n","def getComments():\n","  try:\n","    if not Comments:\n","      raise NameError\n","    else:\n","      return Comments\n","  except NameError:\n","    print (\"Please answer Comments Question\")\n","    return None\n","  \n","\n","def getMentorSupport():\n","  try:\n","    if not Mentor_support:\n","      raise NameError\n","    else:\n","      return Mentor_support\n","  except NameError:\n","    print (\"Please answer Mentor support Question\")\n","    return None\n","\n","def getAnswer():\n","  try:\n","    if not Answer:\n","      raise NameError \n","    else: \n","      return Answer\n","  except NameError:\n","    print (\"Please answer Question\")\n","    return None\n","  \n","\n","def getId():\n","  try: \n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup \n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup() \n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n","\n"],"id":"WBPPuGmBlDIN","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eDTAJ04semY0"},"source":["### Import required packages"],"id":"eDTAJ04semY0"},{"cell_type":"code","metadata":{"id":"x_G-NrfmekZO"},"source":["import numpy as np\n","import pandas as pd \n","from matplotlib import pyplot as plt\n","import scipy\n","from scipy import optimize as opt                                       \n","from scipy.optimize import minimize, fsolve, LinearConstraint  \n","from scipy.linalg import solve "],"id":"x_G-NrfmekZO","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"btNPfWoAMM7B"},"source":["### Convex univariate function optimization\n","\n","A convex function is a continuous function whose value at the midpoint of every interval in its domain does not exceed the arithmetic mean of its values at the ends of the interval.\n","\n","![img](https://mathworld.wolfram.com/images/eps-gif/ConvexFunction_1000.gif)\n","\n","In this case, we will use a simple offset version of the $x^2$ function e.g. a simple parabola (u-shape) function. It is a minimization objective function with an optima at -5.0."],"id":"btNPfWoAMM7B"},{"cell_type":"code","metadata":{"id":"-M7PSeziMM7C"},"source":["# objective function\n","def objective(x):\n","    return (5.0 + x)**2.0\n","\n","# We can plot a coarse grid of this function with input values from -10 to 10 to get an idea of the shape of the target function.\n","\n","# define range\n","r_min, r_max = -10.0, 10.0\n","# prepare inputs\n","inputs = np.arange(r_min, r_max, 0.1)\n","# compute targets\n","targets = [objective(x) for x in inputs]\n","# plot inputs vs target\n","plt.plot(inputs, targets, '--')\n","plt.show()"],"id":"-M7PSeziMM7C","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-BQ6ixjiMM7F"},"source":["Note: in a real optimization problem, we would not be able to perform so many evaluations of the objective function so easily. This simple function is used for demonstration purposes so we can learn how to use the optimization algorithm.\n","\n","Next, we can use the optimization algorithm to find the optima.\n","Once optimized, we can summarize the result, including the input and evaluation of the optima and the number of function evaluations required to locate the optima"],"id":"-BQ6ixjiMM7F"},{"cell_type":"code","metadata":{"id":"rzwxfNufMM7G"},"source":["from scipy.optimize import minimize_scalar\n","\n","result = minimize_scalar(objective)\n","\n","# summarize the result\n","opt_x, opt_y = result['x'], result['fun']\n","print('Optimal Input x: %.6f' % opt_x)\n","print('Optimal Output f(x): %.6f' % opt_y)\n","print('Total Evaluations n: %d' % result['nfev'])\n","\n","# Finally, we can plot the function again and mark the optima to confirm it was located in the place we expected for this function.\n","\n","# define the range\n","r_min, r_max = -10.0, 10.0\n","# prepare inputs\n","inputs = np.arange(r_min, r_max, 0.1)\n","# compute targets\n","targets = [objective(x) for x in inputs]\n","# plot inputs vs target\n","plt.plot(inputs, targets, '--')\n","plt.xlabel(\"inputs\")\n","plt.ylabel(\"targets\")\n","# plot the optima\n","plt.plot([opt_x], [opt_y], 's', color='r')\n","# show the plot\n","plt.show()"],"id":"rzwxfNufMM7G","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7-PFBSteMM7H"},"source":["### Non-convex univariate function optimization\n","\n","A Non-convex function is one that does not resemble a basin, meaning that it may have more than one hill or valley.\n","\n","This can make it more challenging to locate the global optima as the multiple hills and valleys can cause the search to get stuck and report a false or local optima instead.\n","\n","We can define a non-convex univariate function as follows. "],"id":"7-PFBSteMM7H"},{"cell_type":"code","metadata":{"id":"AjSd_S79MM7H"},"source":["# objective function\n","def objective(x):\n","    return (x - 2.0) * x * (x + 2.0)**2.0\n","\n","# We can sample above function and create a line plot of input values to objective values.\n","\n","# plot a non-convex univariate function \n","# define range\n","r_min, r_max = -3.0, 2.5\n","# prepare inputs\n","inputs = np.arange(r_min, r_max, 0.1)\n","# compute targets\n","targets = [objective(x) for x in inputs]\n","# plot inputs vs target\n","plt.plot(inputs, targets, '--')\n","plt.xlabel(\"inputs\")\n","plt.ylabel(\"targets\")\n","plt.show()"],"id":"AjSd_S79MM7H","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D-ioXvLWMM7I"},"source":["# Now let's summarize the result\n","\n","# minimize the function\n","result = minimize_scalar(objective, method='brent')\n","# summarize the result\n","opt_x, opt_y = result['x'], result['fun']\n","print('Optimal Input x: %.6f' % opt_x)\n","print('Optimal Output f(x): %.6f' % opt_y)\n","print('Total Evaluations n: %d' % result['nfev'])\n","\n","# plot the optima on a line plot.\n","# define the range\n","r_min, r_max = -3.0, 2.5\n","# prepare inputs\n","inputs = np.arange(r_min, r_max, 0.1)\n","# compute targets\n","targets = [objective(x) for x in inputs]\n","# plot inputs vs target\n","plt.plot(inputs, targets, '--')\n","# plot the optima\n","plt.plot([opt_x], [opt_y], 's', color='r')\n","# show the plot\n","plt.show()"],"id":"D-ioXvLWMM7I","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PHMx0U8iMM7J"},"source":["### Unconstrained multi variable optimization\n","\n","Multivariate optimization with no constraints is known as unconstrained multivariate optimization.\n","\n","For e.g. $min f(x̄)$ w.r.t $x̄$ where $x̄ ∈ R^n$\n","\n","So, when you look at this optimization problem you typically write it in this above form where you say you are going to minimize $f(x̄)$, and this function is called the objective function. And the variable that you can use to minimize this function is called the decision variable is written as w.r.t $x̄$ here and you also say $x̄$ is continuous that is, it could take any value in the real number line.\n","\n","The necessary and sufficient conditions for $x̄*$ to be the minimizer of the function $f(x̄*)$"],"id":"PHMx0U8iMM7J"},{"cell_type":"markdown","metadata":{"id":"Nk-uWrGTMM7K"},"source":["We use the minimize() function for the performing minimization on the scalar function. [Rosenbrock function](https://www.sfu.ca/~ssurjano/rosen.html) is a popular test problem for optimization algorithms. Consider the problem of minimizing the Rosenbrock function of  variables:\n","\n","$f(x)=  \\sum_{i=1}^{N-1} 100(x_{i+1} - x_i^2)^2 + (1-x_i)^2$\n","\n","\n","The minimum value is 0. We can achieve this by setting x=1.  We can perform this process using [Nelder-Mead](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-neldermead.html) Simplex Algorithm in SciPy"],"id":"Nk-uWrGTMM7K"},{"cell_type":"code","metadata":{"id":"HXkfYwoBMM7K"},"source":["def rosen(x):\n","    \"\"\"The Rosenbrock function\"\"\"\n","    return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)\n","\n","# minimize routine is used with the Nelder-Mead simplex algorithm\n","x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\n","res = opt.minimize(rosen, x0, method='nelder-mead',options={'xatol': 1e-8, 'disp': True})\n","\n","res.x"],"id":"HXkfYwoBMM7K","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lfw3bllmMM7L"},"source":["### Gradient descent\n","\n","Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function.\n","\n","#### Example: Gradient Descent in 2-D\n","We consider a simple problem, corresponding to the minimization of a 2-D quadratic form$$ f(x_1, x_2) = \\frac{1}{2} { x_1^2 + \\eta x_2^2 } ,$$where $\\eta > 0$ controls the anisotropy, and hence the difficulty, of the problem.\n","\n","Anisotropy parameter $\\eta$."],"id":"Lfw3bllmMM7L"},{"cell_type":"code","metadata":{"id":"jfeVNikzMM7M"},"source":["# Let's create a function f and visualize with a contour plot\n","\n","\n","eta = 4\n","f = lambda x : ( x[0]**2 + eta*x[1]**2 ) / 2\n","\n","# generate background for the function\n","tx = np.linspace(-.3,1,101)\n","ty = np.linspace(-.6,.6,101)\n","[v,u] = np.meshgrid(ty,tx)\n","F = ( u ** 2 + eta * v ** 2 ) / 2\n","# visualize\n","plt.contourf(tx,ty,F.transpose(),10);"],"id":"jfeVNikzMM7M","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dhwpWxESMM7M"},"source":["Calculate the gradient\n","\n","The step size should satisfy $\\tau_k < 2/\\eta$. We use here a constant step size."],"id":"dhwpWxESMM7M"},{"cell_type":"code","metadata":{"id":"6XpZ7FnsMM7N"},"source":["Gradf = lambda x : np.array([x[0],eta*x[1]])\n","tau = 1.6/eta\n","# Initial point for the descent.\n","x0 = np.array( [.9,.3] )"],"id":"6XpZ7FnsMM7N","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2Ad1q9pAMM7O"},"source":["Perform the gradient descent using a fixed step size $\\tau_k=\\tau$. Display the decay of the energy $f(x^{(k)})$ through the iteration. Save the iterates so that X(:,k) corresponds to $x^{(k)}$."],"id":"2Ad1q9pAMM7O"},{"cell_type":"code","metadata":{"id":"70E8Rk4nMM7O"},"source":["# iterate and perform gradient descent\n","x = x0\n","niter = 20\n","E = np.zeros((niter,1))\n","X = np.zeros((2,niter))\n","for i in np.arange(0,niter):\n","    X[:,i] = x\n","    E[i] = f(x)\n","    x = x - tau*Gradf(x) # NEW VALUE = OLD VALUE - COEFFICIENT. CORRECTION\n","#visualize\n","plt.plot(np.log10(E))\n","plt.title('$log_{10}f(x^{(k)})$')\n","plt.show()"],"id":"70E8Rk4nMM7O","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FZW7SdQDMM7P"},"source":["Display the iterations using visualization"],"id":"FZW7SdQDMM7P"},{"cell_type":"code","metadata":{"id":"6nVQEBzyMM7P"},"source":["# visualize the countour plot\n","plt.contourf(tx,ty,F.transpose(),10);\n","plt.plot(X[0,:], X[1,:], 'k.-');"],"id":"6nVQEBzyMM7P","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HldOC8EpMM7P"},"source":["Visualization with different step sizes"],"id":"HldOC8EpMM7P"},{"cell_type":"code","metadata":{"id":"hEyfsb9gMM7Q"},"source":["# with different step sizes\n","niter = 20\n","plt.contourf(tx,ty,F.transpose(),10)\n","tau_list = np.array([.5, 1, 1.5, 1.9]) / eta\n","for itau in np.arange(0,tau_list.size):\n","    tau = tau_list[itau]\n","    x = x0\n","    X = np.zeros((2,niter))\n","    for i in np.arange(0,niter):\n","        X[:,i] = x.flatten()\n","        x = x - tau*Gradf(x)\n","    plt.plot(X[0,:], X[1,:], '.-',label=tau)\n","plt.legend()\n","plt.show()"],"id":"hEyfsb9gMM7Q","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P_jt4hMqMM7Q"},"source":["### Other Variants of Gradient Descent"],"id":"P_jt4hMqMM7Q"},{"cell_type":"markdown","metadata":{"id":"jNW91KF6MM7R"},"source":["### Batch Gradient Descent\n","\n","Batch Gradient Descent is when we sum up over all examples on each iteration when performing the updates to the parameters. Therefore, for each update, we have to sum over all examples:\n","\n","$$w = w - \\alpha \\triangledown_w J(w)$$"],"id":"jNW91KF6MM7R"},{"cell_type":"markdown","metadata":{"id":"WyZK8G4FMM7R"},"source":["Example:\n","\n","Here, we consider a simple pendulum oscillation dataset with two variables. The dataset consists of two columns and 89 rows. Each column represents a characteristic of a simple pendulum i.e l (length) and t (time period). The dataset describes the relationship between the l and t which is $L∝T^2$ ."],"id":"WyZK8G4FMM7R"},{"cell_type":"code","metadata":{"id":"Vpak7aPuMM7S"},"source":["# Load the data by using pandas read_csv()\n","data = pd.read_csv(\"SIMPLEPENDULUMOSCILLATIONDATA.csv\")\n","\n","# Get the length and time period values from the dataset\n","l = data['l'].values\n","t = data['t'].values\n","# Get the square of time period\n","tsq = t * t"],"id":"Vpak7aPuMM7S","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EyVlFZYIMM7S"},"source":["The below function `train()` updates the values of m and c and calculates error. The loss is minimized due to the changed values of m and c. The new values m, c and the minimized error is returned."],"id":"EyVlFZYIMM7S"},{"cell_type":"code","metadata":{"id":"5gSibo8mMM7T"},"source":["# function to update m and c\n","def train(x, y, m, c, eta):\n","    const = - 2.0/len(y)\n","    ycalc = m * x + c\n","    delta_m = const * sum(x * (y - ycalc))\n","    delta_c = const * sum(y - ycalc)\n","    m = m - delta_m * eta\n","    c = c - delta_c * eta\n","    error = sum((y - ycalc)**2)/len(y)\n","    return m, c, error"],"id":"5gSibo8mMM7T","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i8NwIKZ9MM7T"},"source":["Let us vary LR and find how the error decreases in each case, and how the final line looks, by training each case for the same number of iterations - 2000."],"id":"i8NwIKZ9MM7T"},{"cell_type":"code","metadata":{"id":"FD9EhSTBMM7T"},"source":["# Save errors\n","errs_1 = []\n","m, c = 0, 0\n","eta = 0.1\n","# Call the train() method for 2000 iterations to update m and c and get error value with eta = 0.1.\n","for iteration in range(2000):\n","    m, c, error = train(l, tsq, m, c, eta)\n","    errs_1.append(error)\n","\n","# Save final line\n","m_1, c_1 = m, c\n","\n","\n","# Visualize the function\n","# Find the lines\n","y_1 = m_1 * l + c_1\n","plt.plot(l, tsq, '.k');\n","plt.plot(l, y_1, \"g\");"],"id":"FD9EhSTBMM7T","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"na6ycDeyMM7U"},"source":["### Stochastic gradient descent:\n","\n","Instead of going through all examples, Stochastic Gradient Descent (SGD) performs the parameters update on each example $(x_i,y_i)$. \n","\n","Here, learning happens on every example:\n","$$w = w - \\alpha \\triangledown_w J(x_i,y_i;w)$$\n","\n","given the equation $y_i = mx_i + c$\n","\n","we calculate the error as $E$ = $(y - y_i)^2$ ; where $y$ is the ground truth and $y_i$ is the prediction\n","\n","Finding the rate of change in error with respect to m is $\\frac{\\partial E_i }{\\partial m}$ = $ -2(y_i - (mx_i + c)) * x_i$\n","\n","rate of change in c is $\\frac{\\partial E_i }{\\partial c}$ = $ -2(y_i - (mx_i + c))$\n","\n","And then we update the slope and bias with change in slope $\\Delta m$ and change in bias $\\Delta c$ with learning rate $eta$\n","\n","$m$  = $m - \\Delta m * eta$\n","\n","$c$  = $c - \\Delta c * eta$\n","\n","The below function `next_step()` updates the values of m and c and calculates error. The loss is minimized due to the changed values of m and c. The new values m, c and the minimized loss is returned."],"id":"na6ycDeyMM7U"},{"cell_type":"code","metadata":{"id":"ju8BfuQGMM7U"},"source":["# change in m and c\n","def next_step(x, y, m, c, eta):\n","    ycalc = m * x + c\n","    error = (y - ycalc) ** 2\n","    delta_m = -(y - ycalc) * x\n","    delta_c = -(y - ycalc)\n","    m = m - delta_m * eta\n","    c = c - delta_c * eta\n","    return m, c, error"],"id":"ju8BfuQGMM7U","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eBPYGrTvMM7V"},"source":["The function below takes a random data point at a time and udpates m and c using the function `next_step()`"],"id":"eBPYGrTvMM7V"},{"cell_type":"code","metadata":{"id":"3Jv9Loz6MM7V"},"source":["# update m and c for one data point\n","def one_loop_random(x, y, m, c, eta):\n","    # Making random idx\n","    random_idx = np.arange(len(y))\n","    np.random.shuffle(random_idx)\n","    # Training with random idx\n","    for idx in random_idx:\n","        m, c, e = next_step(x[idx], y[idx], m, c, eta)\n","        #print(m, c, e)\n","    return m,c,e"],"id":"3Jv9Loz6MM7V","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LAFS6luWMM7W"},"source":["The function below trains the data for 1000 iterations. In each iteration it calls the `one_loop_random()` function."],"id":"LAFS6luWMM7W"},{"cell_type":"code","metadata":{"id":"fP2ljG6sMM7W"},"source":["# train for 1000 iterations\n","def train_stochastic(x, y, m, c, eta, iterations=1000):\n","    for iteration in range(iterations):\n","        m, c, err = one_loop_random(x, y, m, c, eta)\n","    return m, c, err"],"id":"fP2ljG6sMM7W","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NyvkTWz2MM7W"},"source":["# Init m, c\n","m, c = 0, 0\n","# Learning rate\n","lr = 0.001"],"id":"NyvkTWz2MM7W","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uoGQHAwzMM7W"},"source":["plot Errors vs Iterations"],"id":"uoGQHAwzMM7W"},{"cell_type":"code","metadata":{"id":"feF-E6OEMM7X"},"source":["# Call the train_stochastic() method to update m and c and get error value with lr = 0.001.\n","for num in range(10):\n","    m, c, error = train_stochastic(l, tsq, m, c, lr, iterations=100)\n","    print(\"m = {0:.6} c = {1:.6} Error = {2:.6}\".format(m, c, error))\n","    y = m * l + c\n","plt.plot(l, tsq, '.k')\n","plt.plot(l, y)\n","plt.show()"],"id":"feF-E6OEMM7X","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ntb78OTiMM7X"},"source":["**Ungraded Exercise:** Experiment with other lr values."],"id":"Ntb78OTiMM7X"},{"cell_type":"code","metadata":{"id":"pcCW11lJMM7X"},"source":["# YOUR CODE HERE"],"id":"pcCW11lJMM7X","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ObYzud4cr8I8"},"source":["### Constrained Optimization\n","\n","Optimization with constrains on the objective.\n","\n","Example: Optimizing Number of shares in Stock Markets"],"id":"ObYzud4cr8I8"},{"cell_type":"code","metadata":{"id":"_CfHcS0Xr8I8"},"source":["# set variables to determine the number of buyers in the market and \n","# the number of shares you want to sell\n","n_buyers = 10\n","n_shares = 15\n","\n","# Next, create arrays to store the price that each buyer pays, the maximum amount they can afford to spend, and the maximum number of shares each buyer can afford, given the first two arrays.\n","np.random.seed(10)\n","\n","# Generating the array of prices the buyers will pay\n","# np.random.random() creates an array of random numbers\n","# on the half-open interval [0,1)\n","prices = np.random.random(n_buyers)\n","\n","# generate an array of integers on the half-open interval from [1, 4), \n","# again with the size of the number of buyers\n","money_available = np.random.randint(1, 4, n_buyers)"],"id":"_CfHcS0Xr8I8","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fLzttvvjr8I9"},"source":["#### Compute the maximum number of shares each buyer can purchase"],"id":"fLzttvvjr8I9"},{"cell_type":"code","metadata":{"id":"3aaBTGO3r8I-"},"source":["# take the ratio of the money_available with prices to determine the maximum number of shares each buyer can purchase\n","n_shares_per_buyer = money_available / prices\n","\n","# print each of these arrays separated by a newline\n","print(prices, money_available, n_shares_per_buyer, sep=\"\\n\")"],"id":"3aaBTGO3r8I-","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q3MtqolIr8I-"},"source":["# create an array of ones with the length `n_buyers` and pass it as the first argument to `LinearConstraint`\n","constraint = LinearConstraint(np.ones(n_buyers), lb=n_shares, ub=n_shares)"],"id":"q3MtqolIr8I-","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P1twNSIvr8I-"},"source":["Since `LinearConstraint` takes the dot product of the solution vector with this argument, it’ll result in the sum of the purchased shares.\n","\n","This result is then constrained to lie between the other two arguments:\n","\n","1. The lower bound lb\n","2. The upper bound ub\n","\n","Next, create the bounds for the solution variable. The bounds limit the number of shares purchased to be 0 on the lower side and `n_shares_per_buyer` on the upper side. The format that `minimize()` expects for the bounds is a sequence of tuples of lower and upper bounds:"],"id":"P1twNSIvr8I-"},{"cell_type":"code","metadata":{"id":"LXe-zBHVr8I_"},"source":["# use a comprehension to generate a list of tuples for each buyer\n","bounds = [(0, n) for n in n_shares_per_buyer]\n","\n","#define the objective function\n","def objective_function(x, prices):\n","    return -x.dot(prices)"],"id":"LXe-zBHVr8I_","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7nE6-vu2r8I_"},"source":["In above cell, you define `objective_function()` to take two arguments. Then you take the dot product of x with prices and return the negative of that value. Remember that you have to return the negative because you’re trying to make that number as small as possible, or as close to negative infinity as possible. Finally, you can call `minimize()`:"],"id":"7nE6-vu2r8I_"},{"cell_type":"code","metadata":{"id":"X7dMfxS3r8JA"},"source":["res = minimize(\n","    objective_function,\n","    x0=10 * np.random.random(n_buyers),\n","    args=(prices,),\n","    constraints=constraint,\n","    bounds=bounds,\n",")\n","\n","print(res)"],"id":"X7dMfxS3r8JA","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fJlipOm_r8JA"},"source":["In this code, res is an instance of OptimizeResult, just like with `minimize_scalar()`. As you’ll see, there are many of the same fields, even though the problem is quite different. In the call to `minimize()`, you pass five arguments:\n","\n","1. objective_function\n","2. $x_0$\n","3. args\n","4. constraints\n","5. bounds\n","\n","Once the solver runs, you should inspect `res` by printing it:"],"id":"fJlipOm_r8JA"},{"cell_type":"markdown","metadata":{"id":"VHfHdGCP_n6Y"},"source":["### Please answer the questions below to complete the experiment:\n","\n","\n"],"id":"VHfHdGCP_n6Y"},{"cell_type":"markdown","metadata":{"id":"Y8g5UtFCBWET"},"source":["**Question:** Determine the gradient of the function $ f(x,y) = x^2 -2y^2 -4y + 6$ at the point (0, 0). $\\nabla f = [\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}]$\n","\n","**Options:**\n","- a) $\\nabla f = [0, 0]$\n","- b) $\\nabla f = [0, -4]$\n","- c) $\\nabla f = [-2, 4]$\n","- d) $\\nabla f = [4, -2]$"],"id":"Y8g5UtFCBWET"},{"cell_type":"code","metadata":{"id":"VgSwVENIPcM6"},"source":["# @title Please select the correct option below { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Answer = \"\" #@param [\"\", \"a\", \"b\",\"c\",\"d\"]"],"id":"VgSwVENIPcM6","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NMzKSbLIgFzQ"},"source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"id":"NMzKSbLIgFzQ","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DjcH1VWSFI2l"},"source":["#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"\" #@param {type:\"string\"}\n"],"id":"DjcH1VWSFI2l","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4VBk_4VTAxCM"},"source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"],"id":"4VBk_4VTAxCM","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XH91cL1JWH7m"},"source":["#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"id":"XH91cL1JWH7m","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z8xLqj7VWIKW"},"source":["#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"id":"z8xLqj7VWIKW","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FzAZHt1zw-Y-","cellView":"form"},"source":["#@title Run this cell to submit your notebook for grading { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id = return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"id":"FzAZHt1zw-Y-","execution_count":null,"outputs":[]}]}